{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AT_USER hey cici sweetheart! just wanted to let u know i luv u! oh! and will the mixtape drop soon? fantasy ride may 5th!!!! \n",
      "AT_USER i heard about that contest! congrats girl!! \n",
      "unc!!! ncaa champs!! franklin st.: i was there!! wild and crazy!!!!!! nothing like it...ever URL \n",
      "do you share more jokes quotes music photos or news articles on facebook or twitter? \n",
      "good night twitter and thelegionofthefallen. 5:45am cimes awfully early! \n",
      "i just finished a 2.66 mi run with a pace of 11'14\"/mi with nike+ gps. nikeplus makeitcount \n",
      "disappointing day. attended a car boot sale to raise some funds for the sanctuary, made a total of 88p after the entry fee - sigh \n",
      "no more taking irish car bombs with strange australian women who can drink like rockstars...my head hurts. \n",
      "just had some bloodwork done. my arm hurts\n"
     ]
    }
   ],
   "source": [
    "#import regex\n",
    "import re\n",
    "\n",
    "#start process_tweet\n",
    "def processTweet(tweet):\n",
    "    # process the tweets\n",
    "\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    return tweet\n",
    "#end\n",
    "\n",
    "#Read the tweets one by one and process it\n",
    "fp = open('sampleTweets.txt', 'r')\n",
    "line = fp.readline()\n",
    "\n",
    "while line:\n",
    "    processedTweet = processTweet(line)\n",
    "    print(processedTweet)\n",
    "    line = fp.readline()\n",
    "#end loop\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey', 'cici', 'luv', 'mixtape', 'drop', 'soon', 'fantasy', 'ride']\n",
      "['heard', 'congrats']\n",
      "['ncaa', 'franklin', 'wild']\n",
      "['share', 'jokes', 'quotes', 'music', 'photos', 'news', 'articles', 'facebook', 'twitter']\n",
      "['night', 'twitter', 'thelegionofthefallen', 'cimes', 'awfully']\n",
      "['finished', 'mi', 'run', 'pace', 'gps', 'nikeplus', 'makeitcount']\n",
      "['disappointing', 'day', 'attended', 'car', 'boot', 'sale', 'raise', 'funds', 'sanctuary', 'total', 'entry', 'fee', 'sigh']\n",
      "['taking', 'irish', 'car', 'bombs', 'strange', 'australian', 'women', 'drink', 'head', 'hurts']\n",
      "['bloodwork', 'arm', 'hurts']\n"
     ]
    }
   ],
   "source": [
    "#initialize stopWords\n",
    "stopWords = []\n",
    "\n",
    "#start replaceTwoOrMore\n",
    "def replaceTwoOrMore(s):\n",
    "    #look for 2 or more repetitions of character and replace with the character itself\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "    return pattern.sub(r\"\\1\\1\", s)\n",
    "#end\n",
    "\n",
    "#start getStopWordList\n",
    "def getStopWordList(stopWordListFileName):\n",
    "    #read the stopwords file and build a list\n",
    "    stopWords = []\n",
    "    stopWords.append('AT_USER')\n",
    "    stopWords.append('URL')\n",
    "\n",
    "    fp = open(stopWordListFileName, 'r')\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        word = line.strip()\n",
    "        stopWords.append(word)\n",
    "        line = fp.readline()\n",
    "    fp.close()\n",
    "    return stopWords\n",
    "#end\n",
    "\n",
    "#start getfeatureVector\n",
    "def getFeatureVector(tweet):\n",
    "    featureVector = []\n",
    "    #split tweet into words\n",
    "    words = tweet.split()\n",
    "    for w in words:\n",
    "        #replace two or more with two occurrences\n",
    "        w = replaceTwoOrMore(w)\n",
    "        #strip punctuation\n",
    "        w = w.strip('\\'\"?,.')\n",
    "        #check if the word stats with an alphabet\n",
    "        val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*$\", w)\n",
    "        #ignore if it is a stop word\n",
    "        if(w in stopWords or val is None):\n",
    "            continue\n",
    "        else:\n",
    "            featureVector.append(w.lower())\n",
    "    return featureVector\n",
    "#end\n",
    "\n",
    "#Read the tweets one by one and process it\n",
    "fp = open('sampleTweets.txt', 'r')\n",
    "line = fp.readline()\n",
    "\n",
    "st = open('stopwords.txt', 'r')\n",
    "stopWords = getStopWordList('stopwords.txt')\n",
    "\n",
    "while line:\n",
    "    processedTweet = processTweet(line)\n",
    "    featureVector = getFeatureVector(processedTweet)\n",
    "    print(featureVector)\n",
    "    line = fp.readline()\n",
    "#end loop\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read the tweets one by one and process it\n",
    "import csv\n",
    "inpTweets =csv.reader(open('sampleTweets.csv', 'r'), delimiter=',', quotechar='|')\n",
    "tweets = []\n",
    "for row in inpTweets:\n",
    "    sentiment = row[0]\n",
    "    tweet = row[1]\n",
    "    processedTweet = processTweet(tweet)\n",
    "    featureVector = getFeatureVector(processedTweet)\n",
    "    tweets.append((featureVector, sentiment));\n",
    "#end loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['hey', 'cici', 'luv', 'mixtape', 'drop', 'soon', 'fantasy', 'ride'],\n",
       "  'positive'),\n",
       " (['heard', 'congrats'], 'positive'),\n",
       " (['ncaa', 'franklin', 'wild'], 'positive'),\n",
       " (['share',\n",
       "   'jokes',\n",
       "   'quotes',\n",
       "   'music',\n",
       "   'photos',\n",
       "   'news',\n",
       "   'articles',\n",
       "   'facebook',\n",
       "   'twitter'],\n",
       "  'neutral'),\n",
       " (['night', 'twitter', 'thelegionofthefallen', 'cimes', 'awfully'], 'neutral'),\n",
       " (['finished', 'mi', 'run', 'pace', 'gps', 'nikeplus', 'makeitcount'],\n",
       "  'neutral'),\n",
       " (['disappointing',\n",
       "   'day',\n",
       "   'attended',\n",
       "   'car',\n",
       "   'boot',\n",
       "   'sale',\n",
       "   'raise',\n",
       "   'funds',\n",
       "   'sanctuary',\n",
       "   'total',\n",
       "   'entry',\n",
       "   'fee',\n",
       "   'sigh'],\n",
       "  'negative'),\n",
       " (['taking',\n",
       "   'irish',\n",
       "   'car',\n",
       "   'bombs',\n",
       "   'strange',\n",
       "   'australian',\n",
       "   'women',\n",
       "   'drink',\n",
       "   'head',\n",
       "   'hurts'],\n",
       "  'negative'),\n",
       " (['bloodwork', 'arm', 'hurts'], 'negative')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featureList = ['hey', 'cici', 'luv', 'mixtape', 'drop', 'soon', 'fantasy', 'ride', 'heard',\n",
    "'congrats', 'ncaa', 'franklin', 'wild', 'share', 'jokes', 'quotes', 'music', 'photos', 'news',\n",
    "'articles', 'facebook', 'twitter', 'night', 'twitter', 'thelegionofthefallen', 'cimes', 'awfully',\n",
    "'finished', 'mi', 'run', 'pace', 'gps', 'nikeplus', 'makeitcount', 'disappointing', 'day', 'attended',\n",
    "'car', 'boot', 'sale', 'raise', 'funds', 'sanctuary', 'total', 'entry', 'fee', 'sigh', 'taking',\n",
    "'irish', 'car', 'bombs', 'strange', 'australian', 'women', 'drink', 'head', 'hurts', 'bloodwork',\n",
    "'arm', 'hurts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#start extract_features\n",
    "def extract_features(tweet):\n",
    "    tweet_words = set(tweet)\n",
    "    features = {}\n",
    "    for word in featureList:\n",
    "        features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return features\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read the tweets one by one and process it\n",
    "import nltk\n",
    "inpTweets = csv.reader(open('sampleTweets.csv', 'r'), delimiter=',', quotechar='|')\n",
    "stopWords = getStopWordList('stopwords.txt')\n",
    "featureList = []\n",
    "\n",
    "# Get tweet words\n",
    "tweets = []\n",
    "for row in inpTweets:\n",
    "    sentiment = row[0]\n",
    "    tweet = row[1]\n",
    "    processedTweet = processTweet(tweet)\n",
    "    featureVector = getFeatureVector(processedTweet)\n",
    "    featureList.extend(featureVector)\n",
    "    tweets.append((featureVector, sentiment));\n",
    "#end loop\n",
    "\n",
    "# Remove featureList duplicates\n",
    "featureList = list(set(featureList))\n",
    "\n",
    "# Extract feature vector for all tweets in one shote\n",
    "training_set = nltk.classify.util.apply_features(extract_features, tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "#Train the classifier\n",
    "NBClassifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "# Test the classifier\n",
    "testTweet = 'Congrats @ravikiranj, i heard you wrote a new tech post on sentiment analysis'\n",
    "processedTestTweet = processTweet(testTweet)\n",
    "print(NBClassifier.classify(extract_features(getFeatureVector(processedTestTweet))))\n",
    "\n",
    "#Output\n",
    "#======\n",
    "#positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "           contains(car) = False          neutra : negati =      2.3 : 1.0\n",
      "         contains(hurts) = False          neutra : negati =      2.3 : 1.0\n",
      "       contains(twitter) = False          negati : neutra =      2.3 : 1.0\n",
      "      contains(finished) = False          negati : neutra =      1.4 : 1.0\n",
      "         contains(drink) = False          neutra : negati =      1.4 : 1.0\n",
      "         contains(raise) = False          neutra : negati =      1.4 : 1.0\n",
      "         contains(cimes) = False          negati : neutra =      1.4 : 1.0\n",
      "      contains(facebook) = False          negati : neutra =      1.4 : 1.0\n",
      "      contains(congrats) = False          negati : positi =      1.4 : 1.0\n",
      "           contains(luv) = False          negati : positi =      1.4 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# print informative features about the classifier\n",
    "print(NBClassifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "testTweet = 'I am so badly hurt'\n",
    "processedTestTweet = processTweet(testTweet)\n",
    "print(NBClassifier.classify(extract_features(getFeatureVector(processedTestTweet))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (10 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.333\n",
      "             2          -0.86350        1.000\n",
      "             3          -0.69357        1.000\n",
      "             4          -0.57184        1.000\n",
      "             5          -0.48323        1.000\n",
      "             6          -0.41705        1.000\n",
      "             7          -0.36625        1.000\n",
      "             8          -0.32624        1.000\n",
      "             9          -0.29401        1.000\n",
      "         Final          -0.26751        1.000\n",
      "positive\n",
      "  -0.269 Correction feature (58)\n",
      "   0.192 contains(bloodwork)==True and label is 'negative'\n",
      "   0.192 contains(arm)==True and label is 'negative'\n",
      "   0.168 contains(heard)==True and label is 'positive'\n",
      "   0.168 contains(congrats)==True and label is 'positive'\n",
      "   0.152 contains(wild)==True and label is 'positive'\n",
      "   0.152 contains(franklin)==True and label is 'positive'\n",
      "   0.152 contains(ncaa)==True and label is 'positive'\n",
      "   0.147 contains(awfully)==True and label is 'neutral'\n",
      "   0.147 contains(thelegionofthefallen)==True and label is 'neutral'\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Max Entropy Classifier\n",
    "MaxEntClassifier = nltk.classify.maxent.MaxentClassifier.train(training_set, 'GIS', trace=3,encoding=None, labels=None, gaussian_prior_sigma=0, max_iter = 10)\n",
    "testTweet = 'Congrats @ravikiranj, i heard you wrote a new tech post on sentiment analysis'\n",
    "processedTestTweet = processTweet(testTweet)\n",
    "print(MaxEntClassifier.classify(extract_features(getFeatureVector(processedTestTweet))))\n",
    "#print informative features\n",
    "print(MaxEntClassifier.show_most_informative_features(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'svmutil'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-e1c5fcb03bef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0msvmutil\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msvmutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[1;31m#import scikit-learn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;31m#from svm import *\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'svmutil'"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "import svmutil as svmutil\n",
    "#import scikit-learn\n",
    "#from svm import *\n",
    "\n",
    "#training data\n",
    "labels = [0, 1, 1, 2]\n",
    "samples = [[0, 1, 0], [1, 1, 1], [1, 1, 0], [0, 0, 0]]\n",
    "\n",
    "#SVM params\n",
    "param = svm_parameter()\n",
    "param.C = 10\n",
    "param.kernel_type = LINEAR\n",
    "#instantiate the problem\n",
    "problem = svm_problem(labels, samples)\n",
    "#train the model\n",
    "model = svm_train(problem, param)\n",
    "# saved model can be loaded as below\n",
    "#model = svm_load_model('model_file')\n",
    "\n",
    "#save the model\n",
    "svm_save_model('model_file', model)\n",
    "\n",
    "#test data\n",
    "test_data = [[0, 1, 1], [1, 0, 1]]\n",
    "#predict the labels\n",
    "p_labels\n",
    "p_accs\n",
    "p_vals = svm_predict([0]*len(test_data), test_data, model)\n",
    "print(p_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
